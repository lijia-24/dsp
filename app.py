import streamlit as st
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import librosa
import noisereduce as nr
import joblib
import json
import os
import re
from transformers import AutoTokenizer, AutoModel, AutoFeatureExtractor
import plotly.graph_objects as go
import plotly.express as px
import google.generativeai as genai
import shap
from lime.lime_text import LimeTextExplainer
import nltk
from nltk.corpus import stopwords
from sklearn.base import BaseEstimator
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords', quiet=True)
    
# ==================== CONFIGURATION ====================
st.set_page_config(
    page_title="Depression Severity Assessment System",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state
if 'result' not in st.session_state:
    st.session_state.result = None
if 'llm_explanation' not in st.session_state:
    st.session_state.llm_explanation = None
if 'shap_values' not in st.session_state:
    st.session_state.shap_values = None

# Configure Gemini API
try:
    os.environ["GEMINI_API_KEY"] = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=os.environ["GEMINI_API_KEY"])
    GEMINI_AVAILABLE = True
except:
    GEMINI_AVAILABLE = False
    st.warning("âš ï¸ Gemini API key not configured. Using fallback explanations.")

# ==================== GLOBAL CACHE FOR SHAP BACKGROUNDS ====================
AUDIO_BACKGROUND_CACHE = None
FUSION_BACKGROUND_CACHE = None

def get_audio_background():
    """Get or create cached audio background data for SHAP"""
    global AUDIO_BACKGROUND_CACHE
    if AUDIO_BACKGROUND_CACHE is None:
        AUDIO_BACKGROUND_CACHE = np.random.randn(3, 4608) * 0.5
    return AUDIO_BACKGROUND_CACHE

def get_fusion_background():
    """Get or create cached fusion background data for SHAP"""
    global FUSION_BACKGROUND_CACHE
    if FUSION_BACKGROUND_CACHE is None:
        FUSION_BACKGROUND_CACHE = np.array([
            [5.0, 5.0],
            [12.0, 12.0],
            [18.0, 18.0]
        ])
    return FUSION_BACKGROUND_CACHE

# ==================== MODEL DEFINITIONS ====================
device = torch.device("cpu")

class GRURegressor(nn.Module):
    def __init__(self, dim, hidden, layers, dropout):
        super().__init__()
        self.gru = nn.GRU(
            dim, hidden, layers,
            dropout=dropout if layers > 1 else 0,
            bidirectional=True,
            batch_first=True
        )
        self.attn = nn.Sequential(
            nn.Linear(hidden*2, 128),
            nn.Tanh(),
            nn.Linear(128, 1)
        )
        self.reg = nn.Linear(hidden*2, 1)
        self.cls = nn.Linear(hidden*2, 5)
    
    def forward(self, x, mask=None):
        o, _ = self.gru(x)
        attn_logits = self.attn(o).squeeze(-1)
        if mask is not None:
            attn_logits = attn_logits.masked_fill(mask == 0, -1e9)
        w = torch.softmax(attn_logits, dim=1)
        ctx = (w.unsqueeze(-1) * o).sum(dim=1)
        return self.reg(ctx).squeeze(-1), self.cls(ctx)

# ==================== SHAP WRAPPER CLASSES ====================

class AudioModelWrapper(BaseEstimator):
    """Wrapper for audio pipeline to work with SHAP KernelExplainer"""
    def __init__(self, pipeline):
        self.scaler = pipeline['scaler']
        self.selector = pipeline['selector']
        self.pca = pipeline['pca']
        self.svr = pipeline['svr']
    
    def predict(self, X):
        """Takes raw statistical features, returns PHQ-8 predictions"""
        X_scaled = self.scaler.transform(X)
        X_selected = self.selector.transform(X_scaled)
        X_pca = self.pca.transform(X_selected)
        predictions = self.svr.predict(X_pca)
        return np.clip(predictions, 0, 24)

class TextModelWrapper:
    """Wrapper for text model to work with SHAP"""
    def __init__(self, tokenizer, text_encoder, text_model, device):
        self.tokenizer = tokenizer
        self.text_encoder = text_encoder
        self.text_model = text_model
        self.device = device
    
    def predict(self, texts):
        """Takes list of text strings, returns PHQ-8 predictions"""
        predictions = []
        for text in texts:
            score, _ = self._process_single_text(text)
            predictions.append(score)
        return np.array(predictions)
    
    def _process_single_text(self, text):
        """Process single text through the model"""
        text = re.sub(r"http\S+|www\.\S+", " ", text)
        text = re.sub(r"[^a-zA-Z0-9.,?!'\s]", " ", text)
        text = re.sub(r"\s+", " ", text).strip()
        
        inputs = self.tokenizer(text, return_tensors="pt", truncation=False)
        input_ids = inputs['input_ids'][0]
        attention_mask = inputs['attention_mask'][0]
        
        chunks_ids, chunks_masks = [], []
        for i in range(0, len(input_ids), 256):
            c_id = input_ids[i:i+512]
            c_mask = attention_mask[i:i+512]
            if len(c_id) < 512:
                pad = 512 - len(c_id)
                c_id = torch.cat([c_id, torch.full((pad,), self.tokenizer.pad_token_id)])
                c_mask = torch.cat([c_mask, torch.zeros(pad)])
            chunks_ids.append(c_id)
            chunks_masks.append(c_mask)
        
        b_ids = torch.stack(chunks_ids).to(self.device)
        b_mask = torch.stack(chunks_masks).to(self.device)
        
        with torch.no_grad():
            out = self.text_encoder(input_ids=b_ids, attention_mask=b_mask)
            mask_exp = b_mask.unsqueeze(-1).float()
            chunk_embs = (out.last_hidden_state * mask_exp).sum(1) / mask_exp.sum(1).clamp(min=1e-9)
            chunk_mask = torch.ones(1, chunk_embs.size(0)).to(self.device)
            score, _ = self.text_model(chunk_embs.unsqueeze(0), mask=chunk_mask)
        
        return float(score.item()), text

class FusionModelWrapper:
    """Wrapper for fusion model to work with SHAP"""
    def __init__(self, fusion_model):
        self.fusion_model = fusion_model
    
    def predict(self, X):
        """Takes [text_score, audio_score] pairs, returns final PHQ-8"""
        raw_preds = self.fusion_model.predict(X)
        final_preds = []
        for i, pred in enumerate(raw_preds):
            text_val = X[i][0]
            audio_val = X[i][1]
            if text_val > 10.0 and audio_val > 18.0:
                pred += 4.0
            final_preds.append(np.clip(pred, 0, 24))
        return np.array(final_preds)

# ==================== LOAD MODELS ====================
@st.cache_resource
def load_models():
    base_path = "C:\\Users\\User\\OneDrive - Universiti Malaya\\Documents\\Y3S1\\DSP\\Code\\dataproduct"
    
    tokenizer = AutoTokenizer.from_pretrained("rafalposwiata/deproberta-large-depression")
    text_encoder = AutoModel.from_pretrained("rafalposwiata/deproberta-large-depression").to(device)
    text_encoder.eval()
    
    text_model_state = torch.load(f"{base_path}\\text_best_model.pt", map_location=device)
    with open(f"{base_path}\\text_config.json") as f:
        text_cfg = json.load(f)
    
    text_model = GRURegressor(
        dim=text_cfg['input_dim'],
        hidden=text_cfg['hidden_size'],
        layers=text_cfg['num_layers'],
        dropout=text_cfg['dropout']
    ).to(device)
    text_model.load_state_dict(text_model_state)
    text_model.eval()
    
    audio_processor = AutoFeatureExtractor.from_pretrained("ntu-spml/distilhubert")
    audio_encoder = AutoModel.from_pretrained("ntu-spml/distilhubert").to(device)
    audio_encoder.eval()
    
    audio_pipeline = joblib.load(f"{base_path}\\audio_pipeline.pkl")
    fusion_bundle = joblib.load(f"{base_path}\\fusion_deploy_package.pkl")
    
    return tokenizer, text_encoder, text_model, audio_processor, audio_encoder, audio_pipeline, fusion_bundle

try:
    tokenizer, text_encoder, text_model, audio_processor, audio_encoder, audio_pipeline, fusion_bundle = load_models()
    st.success("âœ… All models loaded successfully!")
except Exception as e:
    st.error(f"Fatal error: {e}")
    st.stop()

# ==================== AUDIO PROCESSING ====================
def get_segments_from_transcript(df):
    if 'Start_Time' not in df.columns or 'End_Time' not in df.columns or len(df) == 0:
        return None
    data = df[['Start_Time', 'End_Time']].values
    corrected_segments = []
    for i in range(len(data)):
        curr_start = data[i][0]
        curr_end = data[i][1]
        if i > 0:
            prev_end = corrected_segments[i-1][1]
            if curr_start < prev_end:
                curr_start = prev_end
        corrected_segments.append([curr_start, curr_end])
    return np.array(corrected_segments)

def extract_statistical_features(embeddings):
    if len(embeddings) == 0 or embeddings.shape[0] == 0:
        return np.zeros(768 * 6)
    feat_mean = np.mean(embeddings, axis=0)
    feat_std = np.std(embeddings, axis=0)
    feat_min = np.min(embeddings, axis=0)
    feat_max = np.max(embeddings, axis=0)
    feat_median = np.median(embeddings, axis=0)
    feat_range = feat_max - feat_min
    return np.concatenate([feat_mean, feat_std, feat_min, feat_max, feat_median, feat_range])

def run_audio_inference(wav_path, transcript_df):
    y, sr = librosa.load(wav_path, sr=16000)
    y = nr.reduce_noise(y=y, sr=sr, stationary=True, prop_decrease=0.3)
    
    rms = np.sqrt(np.mean(y**2) + 1e-9)
    current_db = 20 * np.log10(rms + 1e-9)
    gain = -20.0 - current_db
    y = y * (10 ** (gain / 20))
    y = y / max(1.0, np.max(np.abs(y)))
    
    segments = get_segments_from_transcript(transcript_df)
    embeds = []
    
    if segments is None or len(segments) == 0:
        win = int(5.0 * sr)
        hop = int(2.5 * sr)
        segments = [[i/sr, (i+win)/sr] for i in range(0, len(y)-win, hop)]
    
    for s, e in segments:
        s_idx, e_idx = int(s*sr), int(e*sr)
        if s_idx >= len(y):
            break
        clip = y[s_idx:e_idx]
        clip, _ = librosa.effects.trim(clip, top_db=20)
        dur = len(clip) / sr
        if dur < 2.0:
            continue
        
        if dur <= 8.0:
            clips_to_process = [clip]
        else:
            clips_to_process = []
            win_len = int(5.0 * sr)
            hop_len = int(2.5 * sr)
            for start in range(0, len(clip) - win_len + 1, hop_len):
                clips_to_process.append(clip[start:start + win_len])
        
        for c in clips_to_process:
            if len(c) < 100:
                continue
            inputs = audio_processor(c, sampling_rate=sr, return_tensors="pt", padding=True)
            with torch.no_grad():
                out = audio_encoder(inputs.input_values.to(device))
                emb = out.last_hidden_state.mean(dim=1).cpu().numpy().squeeze()
            embeds.append(emb)
    
    if len(embeds) == 0:
        return 0.0, None
    
    embeds_array = np.vstack(embeds)
    stats = extract_statistical_features(embeds_array)
    stats = stats.reshape(1, -1)
    
    raw_stats = stats.copy()
    
    stats = audio_pipeline['scaler'].transform(stats)
    stats = audio_pipeline['selector'].transform(stats)
    stats = audio_pipeline['pca'].transform(stats)
    prediction = audio_pipeline['svr'].predict(stats)[0]
    
    return float(np.clip(prediction, 0, 24)), raw_stats

# ==================== TEXT PROCESSING ====================
def run_text_inference(raw_text):
    wrapper = TextModelWrapper(tokenizer, text_encoder, text_model, device)
    score, clean_text = wrapper._process_single_text(raw_text)
    return score, clean_text

# ==================== OPTIMIZED XAI COMPUTATION ====================
def compute_audio_shap(raw_stats, audio_wrapper):
    """
    Compute SHAP values for audio model using aggregated feature groups.
    
    Problem: 4608 features (768*6) is too high-dimensional for KernelExplainer
    Solution: Aggregate by feature group (Mean, Std, Min, Max, Median, Range)
    This reduces to 6 interpretable features while preserving dimensionality info
    """
    try:
        if raw_stats is None:
            return None
        
        # STEP 1: Aggregate raw_stats into 6 feature groups
        n_features_per_group = 768
        feature_groups = ['Mean', 'Std', 'Min', 'Max', 'Median', 'Range']
        
        aggregated_stats = np.zeros((1, 6))
        for i, group in enumerate(feature_groups):
            start_idx = i * n_features_per_group
            end_idx = start_idx + n_features_per_group
            # Sum absolute values to capture overall contribution magnitude
            aggregated_stats[0, i] = np.sum(np.abs(raw_stats[0, start_idx:end_idx]))
        
        # STEP 2: Create a wrapper that works with aggregated features
        class AggregatedAudioWrapper(BaseEstimator):
            """Converts 6 aggregated features back to full pipeline format"""
            def __init__(self, original_wrapper, n_features=768):
                self.wrapper = original_wrapper
                self.n_features = n_features
            
            def predict(self, X_agg):
                """
                X_agg: (N, 6) - aggregated features [mean_sum, std_sum, ..., range_sum]
                Returns: (N,) - PHQ-8 predictions
                """
                predictions = []
                for row in X_agg:
                    # Reconstruct full feature vector from aggregated values
                    # Distribute aggregated value uniformly across each group
                    X_reconstructed = np.zeros((1, 6 * self.n_features))
                    for i, val in enumerate(row):
                        start_idx = i * self.n_features
                        end_idx = start_idx + self.n_features
                        # Distribute uniformly
                        X_reconstructed[0, start_idx:end_idx] = val / self.n_features
                    
                    # Run through original pipeline
                    pred = self.wrapper.predict(X_reconstructed)[0]
                    predictions.append(pred)
                
                return np.array(predictions)
        
        agg_wrapper = AggregatedAudioWrapper(audio_wrapper)
        
        # STEP 3: Create background data (aggregated)
        background_agg = np.array([
            [50.0, 30.0, 10.0, 100.0, 40.0, 90.0],    # High activity
            [80.0, 50.0, 20.0, 150.0, 70.0, 130.0],   # Very high activity
            [30.0, 15.0, 5.0, 60.0, 25.0, 55.0]       # Low activity
        ])
        
        # STEP 4: Run SHAP on aggregated features
        explainer = shap.KernelExplainer(
            agg_wrapper.predict, 
            background_agg, 
            link="identity"
        )
        
        shap_values = explainer.shap_values(aggregated_stats, nsamples=10, l1_reg="aic")
        
        # STEP 5: Map SHAP values back to feature groups
        aggregated_shap = {}
        feature_groups_full = ['Mean', 'Std', 'Min', 'Max', 'Median', 'Range']
        for i, group in enumerate(feature_groups_full):
            aggregated_shap[group] = float(shap_values[0][i])
        
        return aggregated_shap
        
    except Exception as e:
        st.warning(f"Audio SHAP computation failed: {e}")
        return None


def compute_text_lime(text, text_wrapper):
    """
    Compute LIME explanations for text with stopword removal.
    
    Improvements:
    - Removes English stopwords to focus on meaningful words
    - Filters out pronouns, common words
    - Better word importance interpretation
    """
    try:
        first_person_pronouns = {'i', 'me', 'my', 'myself', 'mine'}
        
        EMOTIONAL_MARKERS = {
            # Mood / Affective
            'sad', 'hopeless', 'worthless', 'lonely', 'miserable', 'unhappy', 'depressed', 
            'empty', 'crying', 'tearful', 'guilty', 'ashamed', 'despair', 'blue',
            
            # Cognitive / Mental State
            'struggle', 'difficult', 'hard', 'impossible', 'useless', 'failure', 'hate', 
            'dark', 'confused', 'lost', 'overwhelmed', 'anxious', 'worried', 'stuck',
            
            # Physical / Somatic
            'tired', 'exhausted', 'heavy', 'sleep', 'insomnia', 'pain', 'ache', 'slow', 
            'weak', 'lethargic', 'appetite', 'weight', 'hunger', 'energy', 'restless',
            
            # Clinical / Intervention
            'medication', 'prescribed', 'therapy', 'doctor', 'pills', 'treatment', 'hospital',
            'diagnosis', 'counseling', 'sessions', 'psychiatrist'
        }
        
        # Add custom common words that don't add clinical value
        custom_noise = {
            # Fillers & Hesitations
            'yeah', 'okay', 'ok', 'uh', 'um', 'right', 'know', 'like', 'sort', 'kind',
            'actually', 'basically', 'literally', 'well', 'anyway', 'so', 'then',
            
            # Non-Clinical Common Verbs
            'go', 'went', 'think', 'thought', 'mean', 'said', 'saying', 'tell', 'told',
            
            # Deictic / Neutral Pointer Words
            'thing', 'things', 'stuff', 'something', 'everything', 'anything', 
            'place', 'time', 'year', 'week', 'day', 'lot', 'maybe', 'probably'
        }
        
        # Download stopwords if not present
        try:
            stopwords.words('english')
        except:
            nltk.download('stopwords', quiet=True)
        
        # Get English stopwords
        stop_words = set(stopwords.words('english'))
        
        clinical_stop_words = stop_words - first_person_pronouns
        
        all_filters = clinical_stop_words | custom_noise
        
        # Create explainer with stopword filtering
        explainer = LimeTextExplainer(
            class_names=['PHQ-8 Score'],
            split_expression=r'\W+'  # Split on whitespace/punctuation
        )
        
        def predict_proba_wrapper(texts):
            """Wrapper for LIME"""
            predictions = text_wrapper.predict(texts)
            normalized = predictions / 24.0
            return np.column_stack([1 - normalized, normalized])
        
        explanation = explainer.explain_instance(
            text, 
            predict_proba_wrapper, 
            num_features=15,  # Get more features initially
            num_samples=10   # Increase samples for stability
        )
        
        word_importance = explanation.as_list()
        
        # STEP 1: Filter out stopwords and non-meaningful words
        filtered_importance = []
        for word, weight in word_importance:
            # Clean word for comparison
            clean_word = word.lower().strip()
            
            # Skip if it's a stopword or too short
            if clean_word not in first_person_pronouns:
                if clean_word in all_filters or len(clean_word) < 2:
                    continue
            
            # Skip if it's mostly punctuation
            if not any(c.isalpha() for c in clean_word):
                continue
            
            if clean_word in EMOTIONAL_MARKERS:
                        weight = weight * 1.5
                        
            filtered_importance.append((word, weight))
        
        # STEP 2: Sort by absolute importance and return top features
        filtered_importance = sorted(filtered_importance, 
                                    key=lambda x: abs(x[1]), 
                                    reverse=True)[:12]
        
        return filtered_importance
        
    except Exception as e:
        st.warning(f"Text LIME computation failed: {e}")
        return None


def compute_fusion_shap(text_score, audio_score, fusion_wrapper):
    """
    Compute SHAP values for fusion model.
    This part remains the same - only 2 features so no issue.
    """
    try:
        background = get_fusion_background()
        explainer = shap.KernelExplainer(
            fusion_wrapper.predict, 
            background, 
            link="identity"
        )
        
        instance = np.array([[text_score, audio_score]])
        shap_values = explainer.shap_values(instance, nsamples=10, l1_reg="aic")
        
        return {
            'text_contribution': float(shap_values[0][0]),
            'audio_contribution': float(shap_values[0][1]),
            'base_value': float(explainer.expected_value)
        }
    except Exception as e:
        st.warning(f"Fusion SHAP computation failed: {e}")
        return None

# ==================== LLM EXPLANATION ====================
def generate_gemini_explanation_with_shap(phq8_score, severity_level, text_score, audio_score, 
                                          audio_shap, text_lime, fusion_shap):
    """Generate comprehensive explanation using real XAI values"""
    
    if not GEMINI_AVAILABLE:
        return generate_fallback_with_shap(phq8_score, severity_level, audio_shap, text_lime, fusion_shap)
    
    # Format XAI findings - AVOID HALLUCINATION
    audio_shap_str = "No audio SHAP values available"
    if audio_shap:
        sorted_audio = sorted(audio_shap.items(), key=lambda x: abs(x[1]), reverse=True)
        audio_shap_str = "\n".join([
            f"- **{feat}**: {val:+.3f} contribution to score"
            for feat, val in sorted_audio[:5]
        ])
    
    text_lime_str = "No text LIME values available"
    if text_lime:
        text_lime_str = "\n".join([
            f"- **'{word}'**: {val:+.3f} impact"
            for word, val in text_lime[:8]
        ])
    
    fusion_shap_str = "No fusion SHAP values available"
    if fusion_shap:
        fusion_shap_str = f"""
- **Text Modality SHAP**: {fusion_shap['text_contribution']:+.3f}
- **Audio Modality SHAP**: {fusion_shap['audio_contribution']:+.3f}
- **Base Value (Expected)**: {fusion_shap['base_value']:.2f}
- **Final Prediction**: {phq8_score:.2f}
"""
    
    guidelines = get_severity_guidelines(severity_level)
    
    # Explain SHAP directionality clearly
    shap_interpretation_guide = f"""
**CRITICAL SHAP INTERPRETATION RULES (You MUST follow these):**

1. **For Audio/Text Features:**
   - **Positive SHAP value** = This feature INCREASED the depression score (pushed toward depression)
   - **Negative SHAP value** = This feature DECREASED the depression score (pushed toward wellness)

2. **For a HEALTHY patient (PHQ < 10):**
   - Features with NEGATIVE SHAP values are PROTECTIVE factors (helped keep score low)
   - Features with POSITIVE SHAP values are RISK factors (tried to increase score, but were overridden)

3. **For a DEPRESSED patient (PHQ >= 10):**
   - Features with POSITIVE SHAP values are RISK factors (pushed score higher)
   - Features with NEGATIVE SHAP values are RESILIENCE factors (tried to lower score, but insufficient)

4. **Current Patient Status:**
   - Final PHQ-8 Score: {phq8_score:.1f}
   - Classification: {"HEALTHY" if phq8_score < 10 else "DEPRESSED"}
   - Therefore: Interpret SHAP values in this context

5. **Small SHAP values (0.001-0.1) are NORMAL:**
   - These indicate a well-calibrated model
   - Every 0.1 SHAP = 0.1 point change in PHQ-8 score
   - Values don't need to be large to be clinically meaningful

6. **NEVER say "positive impact" without clarification:**
   - Instead say: "pushed the score higher" or "increased depression severity"
   - Instead say: "pushed the score lower" or "decreased depression severity"
"""

    prompt = f"""You are a clinical AI assistant analyzing depression assessment results with explainable AI (XAI) insights.

**Assessment Results:**
- Final PHQ-8 Score: {phq8_score:.1f}/24
- Severity Level: {severity_level}
- Patient Status: {"HEALTHY (score < 10)" if phq8_score < 10 else "DEPRESSED (score â‰¥ 10)"}
- Text Prediction: {text_score:.1f}
- Audio Prediction: {audio_score:.1f}

{shap_interpretation_guide}

**XAI Explainability Analysis (Real Feature Attributions):**

ðŸŽ¤ **Audio SHAP Values (Acoustic Features):**
{audio_shap_str}

ðŸ“ **Text LIME Values (Top Words):**
{text_lime_str}

âš–ï¸ **Fusion SHAP Values (Modality Integration):**
{fusion_shap_str}

**CRITICAL INSTRUCTION:** 
- If ANY XAI section says "No ... values available", DO NOT make up or guess values
- Only discuss XAI findings that are actually present in the data above
- Be honest about what data is available and what isn't

**Clinical Context for {severity_level}:**
- Focus: {guidelines['focus']}
- Urgency: {guidelines['urgency']}

---

Provide a comprehensive, evidence-based explanation with these sections:

## 1. Overall Interpretation (4 sentences)
Explain the PHQ-8 score and what it indicates. Reference the severity level and its clinical significance and give simple explanation.

## 2. Audio Analysis - Evidence from Voice (6 sentences)
ONLY if audio SHAP values are available:
- List the top 3 features and their SHAP values
- **CRITICALLY:** Explain whether each pushed TOWARD depression (+) or TOWARD wellness (-)
- For healthy patients: Explain which features protected them (negative SHAP)
- For depressed patients: Explain which features contributed to severity (positive SHAP)
- How voice characteristics relate to depression markers

If no audio SHAP available, state: "Audio feature analysis was not available for this assessment."

## 3. Text Analysis - Evidence from Language (6 sentences)
ONLY if text LIME values are available:
- Top 5 specific words increased/decreased the severity
- What linguistic patterns and context these words represent
- How language use reflects mental state

If no text LIME available, state: "Text feature analysis was not available for this assessment."

## 4. Multimodal Integration (4-5 sentences)
ONLY if fusion SHAP values are available:
- State whether text/audio pushed in same direction or different directions
- Explain how the fusion layer balanced competing signals
- Discuss whether modalities agreed or showed discordance
- Explain why final score differs from base value
- Interpret what this tells us about the patient's presentation

If no fusion SHAP available, state: "Multimodal integration analysis was not available."

## 5. Clinical Insights (5 sentences)
Based on the PHQ-8 score and severity level:
- Synthesize the overall mental health picture
- What is the most likely emotional/mental state?
- Are there specific concerns or protective factors?
- Does the XAI reveal any surprising patterns?
- Validate the person's experience with empathy
- Bridge from data to human understanding

## 6. Evidence-Based Recommendations
Provide 3 specific, actionable recommendations tailored to **{severity_level}** severity.

For each recommendation, format as:
### [Action Title]
**Why this matters:** [2 sentences]

**Specific steps:**
1. [Concrete action]
2. [Concrete action]
3. [Concrete action]

**Timeline:** [When to do this]

## 7. Critical Reminders
- This is screening based on XAI analysis, not diagnosis
- When to seek immediate help (specific to {severity_level})
- Crisis resources

**Tone Requirements:**
- Warm, empathetic, person-first language
- Scientifically accurate but accessible
- Reference ONLY XAI findings that are actually available
- Never hallucinate or invent XAI values
- Validate feelings while being clinically appropriate
"""

    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.7,
                top_p=0.9,
                max_output_tokens=4000,
            )
        )
        return response.text
    except Exception as e:
        st.error(f"Gemini API error: {e}")
        return generate_fallback_with_shap(phq8_score, severity_level, audio_shap, text_lime, fusion_shap)

def generate_fallback_with_shap(phq8_score, severity_level, audio_shap, text_lime, fusion_shap):
    """FIXED Fallback explanation with correct SHAP interpretation"""
    
    is_healthy = phq8_score < 10
    
    explanation = f"""## ðŸ§  Assessment Analysis

### Overall Interpretation
Your PHQ-8 score of **{phq8_score:.1f}/24** places you in the **{severity_level}** range, which is classified as {"**HEALTHY**" if is_healthy else "**CLINICALLY SIGNIFICANT**"}.

### Audio Evidence (SHAP Analysis - Embedding-Derived)
"""
    if audio_shap:
        sorted_audio = sorted(audio_shap.items(), key=lambda x: abs(x[1]), reverse=True)
        
        for feat, val in sorted_audio:
            if val > 0:
                direction = "ðŸ”´ **Pushed score HIGHER** (toward depression)"
                context = "risk factor" if not is_healthy else "risk factor (overridden by other signals)"
            else:
                direction = "ðŸŸ¢ **Pushed score LOWER** (toward wellness)"
                context = "protective factor" if is_healthy else "resilience factor (insufficient to prevent depression)"
            
            explanation += f"\n- **{feat}**: {val:+.4f} | {direction}\n  *Clinical interpretation: {context}*\n"
        
        explanation += "\n*Note: These are learned embedding patterns from HuBERT, not direct acoustic measurements. Small values (0.01-0.1) are normal and clinically meaningful.*\n"
    else:
        explanation += "- Audio feature analysis not available for this sample\n"
    
    explanation += "\n### Text Evidence (LIME Analysis)\n"
    if text_lime:
        for word, val in text_lime[:7]:
            if val > 0:
                direction = "ðŸ”´ **Increased severity**"
                context = "depressive marker" if not is_healthy else "concerning word (impact minimized)"
            else:
                direction = "ðŸŸ¢ **Decreased severity**"
                context = "healthy language" if is_healthy else "positive indicator (insufficient)"
            
            explanation += f"- **'{word}'**: {val:+.4f} | {direction} ({context})\n"
    else:
        explanation += "- Text feature analysis not available for this sample\n"
    
    explanation += "\n### Fusion Analysis (How Modalities Combined)\n"
    
    if fusion_shap:
        t_val = fusion_shap['text_contribution']
        a_val = fusion_shap['audio_contribution']
        base = fusion_shap['base_value']
        
        explanation += f"- **Base Value (Population Average)**: {base:.2f}\n"
        explanation += f"- **Text Contribution**: {t_val:+.4f} ({'pushed score higher' if t_val > 0 else 'pushed score lower'})\n"
        explanation += f"- **Audio Contribution**: {a_val:+.4f} ({'pushed score higher' if a_val > 0 else 'pushed score lower'})\n"
        explanation += f"- **Final Score**: {phq8_score:.2f} = {base:.2f} + {t_val:.4f} + {a_val:.4f}\n\n"
        
        if (t_val > 0 and a_val > 0):
            explanation += "*Both modalities agreed in pushing toward higher severity.*\n"
        elif (t_val < 0 and a_val < 0):
            explanation += "*Both modalities agreed in pushing toward lower severity (wellness).*\n"
        else:
            explanation += "*Modalities showed conflicting signals - fusion layer had to balance competing evidence.*\n"
    else:
        explanation += "- Fusion analysis not available\n"
    
    guidelines = get_severity_guidelines(severity_level)
    explanation += f"\n### Recommended Actions for {severity_level}:\n\n"
    for i, action in enumerate(guidelines['key_actions'], 1):
        explanation += f"{i}. {action}\n"
    
    if severity_level in ["Moderately Severe", "Severe"]:
        explanation += "\n### âš ï¸ Immediate Action Needed\n"
        explanation += "- Contact mental health professional immediately\n"
        explanation += "- Call 988 if in crisis\n"
    
    explanation += "\n### Important\nThis is screening based on XAI analysis. Consult a mental health professional for proper diagnosis."
    return explanation

def get_severity_guidelines(severity_level):
    guidelines = {
        "Normal": {
            "focus": "Maintenance and prevention",
            "urgency": "Low",
            "key_actions": [
                "Continue current positive habits",
                "Build resilience through regular self-care",
                "Stay socially connected"
            ]
        },
        "Mild": {
            "focus": "Early intervention and monitoring",
            "urgency": "Low-Moderate",
            "key_actions": [
                "Increase self-monitoring of mood patterns",
                "Consider talking to a counselor",
                "Implement structured daily routines",
                "Engage in regular physical activity"
            ]
        },
        "Moderate": {
            "focus": "Active intervention needed",
            "urgency": "Moderate",
            "key_actions": [
                "Schedule appointment with mental health professional",
                "Consider evidence-based therapy (CBT)",
                "Reach out to trusted friends/family",
                "Establish consistent sleep schedule"
            ]
        },
        "Moderately Severe": {
            "focus": "Urgent professional support required",
            "urgency": "High",
            "key_actions": [
                "Seek professional help within 1-2 days",
                "Consider medication evaluation",
                "Inform close family/friends",
                "Create safety plan with crisis contacts"
            ]
        },
        "Severe": {
            "focus": "Immediate intervention required",
            "urgency": "Critical",
            "key_actions": [
                "Contact crisis line immediately (988)",
                "Go to emergency department if having suicidal thoughts",
                "Do not stay alone",
                "Contact mental health provider same day"
            ]
        }
    }
    return guidelines.get(severity_level, guidelines["Moderate"])

# ==================== SAMPLE DATA ====================
SAMPLE_MAP = {
    "Sample 626 (Healthy, PHQ=0)": "626",
    "Sample 606 (Healthy, PHQ=5)": "606",
    "Sample 655 (Depressed, PHQ=12)": "655",
    "Sample 716 (Depressed, PHQ=15)": "716",
    "Sample 624 (Depressed, PHQ=22)": "624"
}

# ==================== SIDEBAR ====================
st.sidebar.title("Navigation")
st.sidebar.markdown("---")
page = st.sidebar.radio(
    "Select Page",
    ["ðŸ“Š Introduction", "ðŸ“ˆ Dashboard", "ðŸ”® Prediction"],
    label_visibility="collapsed"
)
st.sidebar.markdown("---")
st.sidebar.info("**Crisis Support:**\n\nðŸ“ž **MIASA:** 1-800-18-0066\n\nðŸ¥ **MMHA:** +603 2780 6803")
        
# ==================== PREDICTION PAGE ====================
if page == "ðŸ”® Prediction":
    st.title("ðŸ”® Depression Severity Prediction with Explainable AI")
    st.markdown("âš¡ **Optimized** multimodal analysis with **LIME + SHAP** for transparent insights")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("1. Patient Selection")
        selected_label = st.selectbox("Select Test Participant:", list(SAMPLE_MAP.keys()))
        pid = SAMPLE_MAP[selected_label]
        
        base_path = "."
        audio_path = os.path.join(base_path, f"{pid}_AUDIO.mp3")
        csv_path = os.path.join(base_path, f"{pid}_Transcript.csv")
        
        file_check = os.path.exists(audio_path) and os.path.exists(csv_path)
        
        if file_check:
            st.audio(audio_path)
            try:
                df_trans = pd.read_csv(csv_path, sep='\t')
                if df_trans.shape[1] < 2:
                    df_trans = pd.read_csv(csv_path, sep=',')
                st.caption("Transcript Preview:")
                st.dataframe(df_trans.head(3), hide_index=True)
            except Exception as e:
                st.error(f"Error: {e}")
                file_check = False
        else:
            st.error(f"âŒ Files not found for {pid}")
    
    with col2:
        st.subheader("2. Run Analysis with XAI")
        if file_check:
            st.info("âœ… Ready for explainable multimodal analysis")
            
            if st.button("ðŸš€ Analyze with LIME + SHAP", type="primary", use_container_width=True):
                with st.status("Running optimized XAI analysis...", expanded=True) as status:
                    try:
                        # Extract text
                        col = 'Text' if 'Text' in df_trans.columns else df_trans.columns[-1]
                        raw_text = " ".join(df_trans[col].astype(str).tolist())
                        
                        # TEXT MODULE
                        st.write("ðŸ“ **Text Module:** Processing linguistic patterns...")
                        t_score, clean_text = run_text_inference(raw_text)
                        st.write(f" âžœ Text Score: **{t_score:.2f}**")
                        
                        # AUDIO MODULE
                        st.write("ðŸŽ¤ **Audio Module:** Extracting acoustic features...")
                        a_score, raw_audio_stats = run_audio_inference(audio_path, df_trans)
                        st.write(f" âžœ Audio Score: **{a_score:.2f}**")
                        
                        # FUSION
                        st.write("âš–ï¸ **Fusion Layer:** Combining modalities...")
                        fusion_wrapper = FusionModelWrapper(fusion_bundle['model'])
                        raw_fusion = fusion_wrapper.predict([[t_score, a_score]])[0]
                        final_score = np.clip(raw_fusion, 0, 24)
                        st.write(f" âžœ Final PHQ-8 Score: **{final_score:.2f}**")
                        
                        # XAI COMPUTATION (PARALLEL EXECUTION FOR SPEED)
                        st.write("ðŸ” **XAI Module:** Computing LIME + SHAP in parallel...")
                        
                        audio_shap = None
                        text_lime = None
                        fusion_shap = None
                        
                        def compute_audio_shap_task():
                            if raw_audio_stats is not None:
                                audio_wrapper = AudioModelWrapper(audio_pipeline)
                                return compute_audio_shap(raw_audio_stats, audio_wrapper)
                            return None
                        
                        def compute_text_lime_task():
                            text_wrapper = TextModelWrapper(tokenizer, text_encoder, text_model, device)
                            return compute_text_lime(clean_text, text_wrapper)
                        
                        def compute_fusion_shap_task():
                            fusion_wrapper_local = FusionModelWrapper(fusion_bundle['model'])
                            return compute_fusion_shap(t_score, a_score, fusion_wrapper_local)
                        
                        # Run XAI computations in parallel (3x speed boost)
                        with ThreadPoolExecutor(max_workers=3) as executor:
                            future_audio = executor.submit(compute_audio_shap_task)
                            future_text = executor.submit(compute_text_lime_task)
                            future_fusion = executor.submit(compute_fusion_shap_task)
                            
                            # Collect results as they complete
                            for future in as_completed([future_audio, future_text, future_fusion]):
                                try:
                                    result = future.result()
                                    if future == future_audio:
                                        audio_shap = result
                                        st.write(" âžœ Audio SHAP computed âœ“")
                                    elif future == future_text:
                                        text_lime = result
                                        st.write(" âžœ Text LIME computed âœ“")
                                    elif future == future_fusion:
                                        fusion_shap = result
                                        st.write(" âžœ Fusion SHAP computed âœ“")
                                except Exception as e:
                                    st.warning(f"XAI task failed: {e}")
                        
                        # Determine severity
                        if final_score < 4.45:
                            sev = "Normal"
                        elif final_score < 9.45:
                            sev = "Mild"
                        elif final_score < 14.45:
                            sev = "Moderate"
                        elif final_score < 19.45:
                            sev = "Moderately Severe"
                        else:
                            sev = "Severe"
                        
                        # Generate LLM explanation with XAI
                        st.write("ðŸ¤– **LLM Module:** Generating evidence-based insights...")
                        llm_explanation = generate_gemini_explanation_with_shap(
                            final_score, sev, t_score, a_score,
                            audio_shap, text_lime, fusion_shap
                        )
                        
                        # Store results
                        st.session_state.result = {
                            'score': final_score,
                            't_contrib': t_score,
                            'a_contrib': a_score,
                            'severity': sev,
                            'clean_text': clean_text
                        }
                        st.session_state.shap_values = {
                            'audio': audio_shap,
                            'text': text_lime,
                            'fusion': fusion_shap
                        }
                        st.session_state.llm_explanation = llm_explanation
                        
                        status.update(label="âœ… XAI Analysis Complete!", state="complete", expanded=False)
                        
                    except Exception as e:
                        st.error(f"âŒ Error during inference: {e}")
                        import traceback
                        st.code(traceback.format_exc())
        else:
            st.warning("âš ï¸ Please ensure all required files are available")
            

    # ==================== RESULTS DISPLAY ====================
    if st.session_state.result and st.session_state.shap_values:
        res = st.session_state.result
        shap_vals = st.session_state.shap_values
        
        st.divider()
        st.markdown("## ðŸ“‹ Comprehensive Assessment Results with Explainability")
        
        # Score Display
        st.markdown("### ðŸ“Š Core Metrics")
        c1, c2, c3 = st.columns(3)
        
        score = res['score']
        sev = res['severity']
        color_map = {
            "Normal": "#10b981",
            "Mild": "#3b82f6",
            "Moderate": "#f59e0b",
            "Moderately Severe": "#ef4444",
            "Severe": "#7f1d1d"
        }
        color = color_map[sev]
        
        with c1:
            st.markdown(
                f"<div style='text-align:center; padding:20px; background:{color}20; border-radius:10px;'>"
                f"<div style='font-size:32px; font-weight:bold; color:{color};'>{score:.1f} / 24</div>"
                f"<div style='color:#666; margin-top:10px;'>PHQ-8 Score</div></div>",
                unsafe_allow_html=True
            )
        
        with c2:
            st.markdown(
                f"<div style='text-align:center; padding:20px; background:{color}20; border-radius:10px;'>"
                f"<div style='font-size:32px; font-weight:bold; color:{color};'>{sev}</div>"
                f"<div style='color:#666; margin-top:10px;'>Severity Level</div></div>",
                unsafe_allow_html=True
            )
        
        with c3:
            status_label = "âš ï¸ Depression Likely" if score >= 10 else "âœ… Healthy Range"
            status_color = "#ef4444" if score >= 10 else "#10b981"
            st.markdown(
                f"<div style='text-align:center; padding:20px; background:{status_color}20; border-radius:10px;'>"
                f"<div style='font-size:32px; font-weight:bold; color:{status_color};'>{status_label}</div>"
                f"<div style='color:#666; margin-top:10px;'>Screening Result</div></div>",
                unsafe_allow_html=True
            )
        
        st.markdown("---")
        
        # SHAP Visualizations
        st.markdown("### ðŸ”¬ Explainable AI: SHAP Feature Attributions")
        st.markdown("**What influenced this prediction?** Real SHAP values show how each feature contributed.")
        
        tab_fusion, tab_audio, tab_text = st.tabs(["âš–ï¸ Fusion SHAP", "ðŸŽ¤ Audio SHAP", "ðŸ“ Text SHAP"])
        
        with tab_fusion:
            st.markdown("#### Multimodal Integration Analysis")
            if shap_vals['fusion']:
                fusion = shap_vals['fusion']
                
                # Create waterfall-style visualization
                fig = go.Figure()
                
                base = fusion['base_value']
                text_contrib = fusion['text_contribution']
                audio_contrib = fusion['audio_contribution']
                
                # Waterfall data
                categories = ['Base Value', 'Text Impact', 'Audio Impact', 'Final Score']
                values = [base, text_contrib, audio_contrib, score]
                
                colors = ['#94a3b8', 
                         '#ef4444' if text_contrib > 0 else '#10b981',
                         '#ef4444' if audio_contrib > 0 else '#10b981',
                         color]
                
                fig.add_trace(go.Bar(
                    x=categories,
                    y=values,
                    marker_color=colors,
                    text=[f"{v:.2f}" for v in values],
                    textposition='outside'
                ))
                
                fig.update_layout(
                    title="SHAP Fusion Analysis: How Modalities Combined",
                    yaxis_title="PHQ-8 Score",
                    height=400,
                    showlegend=False,
                    template="plotly_white"
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                col_expl1, col_expl2 = st.columns(2)
                with col_expl1:
                    st.metric("Expected Value (Base)", f"{base:.2f}", 
                             help="Average prediction across all patients")
                    st.metric("Text Modality SHAP", f"{text_contrib:+.2f}",
                             help="How linguistic patterns shifted the prediction")
                
                with col_expl2:
                    st.metric("Audio Modality SHAP", f"{audio_contrib:+.2f}",
                             help="How acoustic patterns shifted the prediction")
                    st.metric("Final Prediction", f"{score:.2f}",
                             help="Base + Text + Audio contributions")
                
                st.info("""
                **ðŸ’¡ Understanding Fusion SHAP:**
                - **Base Value**: The average prediction the model would make without seeing your data
                - **Positive SHAP**: Feature increases depression severity
                - **Negative SHAP**: Feature decreases depression severity
                - **Final = Base + Text SHAP + Audio SHAP**
                """)
        
        with tab_audio:
            st.markdown("#### Acoustic Feature Contributions")
            if shap_vals['audio']:
                audio_shap = shap_vals['audio']
                
                # Sort by absolute contribution
                sorted_audio = sorted(audio_shap.items(), key=lambda x: abs(x[1]), reverse=True)
                
                features = [f[0] for f in sorted_audio]
                values = [f[1] for f in sorted_audio]
                colors_audio = ['#ef4444' if v > 0 else '#10b981' for v in values]
                
                fig = go.Figure(go.Bar(
                    y=features,
                    x=values,
                    orientation='h',
                    marker_color=colors_audio,
                    text=[f"{v:+.3f}" for v in values],
                    textposition='outside'
                ))
                
                fig.update_layout(
                    title="Audio SHAP Values: Statistical Feature Contributions",
                    xaxis_title="SHAP Value (Impact on PHQ-8)",
                    yaxis_title="Acoustic Feature",
                    height=400,
                    template="plotly_white"
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                st.markdown("**Feature Explanations:**")
                # 1. Define a clinical mapping for the 6 feature groups
                CLINICAL_MAPPING = {
                    'Mean': {'name': 'Vocal Energy', 'desc': 'overall intensity and breath support in speech'},
                    'Std': {'name': 'Vocal Prosody', 'desc': 'the melodic variation and emotional inflection'},
                    'Range': {'name': 'Dynamic Reach', 'desc': 'the breadth between highest and lowest emotional tones'},
                    'Max': {'name': 'Peak Emphasis', 'desc': 'the ability to project or emphasize specific words'},
                    'Min': {'name': 'Vocal Floor', 'desc': 'the lowest pitch frequencies reached during speech'},
                    'Median': {'name': 'Pitch Baseline', 'desc': 'the habitual fundamental frequency of the voice'}
                }

                for feat, val in sorted_audio[:3]:
                    # Get clinical info from our map
                    info = CLINICAL_MAPPING.get(feat, {'name': feat, 'desc': 'acoustic patterns'})
                    
                    # Determine visual cues
                    if val > 0:
                        direction = "ðŸ”´ **Increased**"
                        impact_desc = "contributing to depressive markers"
                        trend = "higher"
                    else:
                        direction = "ðŸŸ¢ **Decreased**"
                        impact_desc = "indicating emotional wellness"
                        trend = "lower"

                    # Display clean, professional insight
                    st.markdown(f"""
                    **{info['name']} ({feat})**: {direction} severity by **{abs(val):.2f}** points.  
                    *Clinical Insight:* The model detected {trend} than average {info['desc']}, {impact_desc}.
                    """)

                st.divider()

                st.info("""
                **ðŸ“– Feature Dictionary**
                * **Vocal Energy (Mean):** Relates to speech drive. Low energy is often associated with lethargy or psychomotor retardation.
                * **Vocal Prosody (Std):** Relates to 'flat affect.' Reduced variability (monotone speech) is a classic objective marker of depression.
                * **Dynamic Reach (Range):** Measures emotional expressiveness. A narrow range suggests emotional blunting or restricted affect.
                * **Vocal Ceiling (Max):** Indicates the highest level of emotional arousal or intensity reached. Low maximums suggest an inability to project excitement or emphasis.
                * **Vocal Floor (Min):** Identifies 'vocal fry' or creaky voice. Frequent low-frequency drops can be linked to low speech drive.
                * **Pitch Baseline (Median):** Represents the habitual "default" tone. Significant shifts from a normal median can indicate changes in physiological arousal levels.
                
                Lower energy, reduced variability, and narrow range often indicate depression.

                """)

        with tab_text:
            st.markdown("#### Word-Level Linguistic Contributions")
            if shap_vals['text']:
                text_shap = shap_vals['text']
                
                words = [w[0] for w in text_shap[:10]]
                values = [w[1] for w in text_shap[:10]]
                colors_text = ['#ef4444' if v > 0 else '#10b981' for v in values]
                
                fig = go.Figure(go.Bar(
                    y=words,
                    x=values,
                    orientation='h',
                    marker_color=colors_text,
                    text=[f"{v:+.3f}" for v in values],
                    textposition='outside'
                ))
                
                fig.update_layout(
                    title="Text SHAP Values: Top Word Contributions",
                    xaxis_title="SHAP Value (Impact on PHQ-8)",
                    yaxis_title="Word",
                    height=500,
                    template="plotly_white"
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Categorize words
                positive_words = [(w, v) for w, v in text_shap[:10] if v < -0.005]
                negative_words = [(w, v) for w, v in text_shap[:10] if v > 0.005]
                
                col_neg, col_pos = st.columns(2)
                
                with col_neg:
                    st.markdown("**ðŸ”´ Words Increasing Severity:**")
                    for word, val in negative_words[:5]:
                        st.markdown(f"- **'{word}'**: +{abs(val):.3f}")
                
                with col_pos:
                    st.markdown("**ðŸŸ¢ Words Decreasing Severity:**")
                    for word, val in positive_words[:5]:
                        st.markdown(f"- **'{word}'**: {val:.3f}")
                
                st.info("""
                **ðŸ’¡ Understanding Word SHAP:**
                - **Positive SHAP (Red)**: Words associated with depressive language
                - **Negative SHAP (Green)**: Words associated with healthy expression
                - Values show the magnitude of each word's contribution
                - Model learned these patterns from clinical data
                """)
            else:
                st.warning("Text SHAP computation is in progress or unavailable.")
        
        st.markdown("---")
        
        # LLM Explanation
        st.markdown("### ðŸ¤– AI-Generated Clinical Insights")
        st.markdown(f"**Evidence-Based Analysis** | Severity: **{sev}** | PHQ-8: **{score:.1f}/24**")
        
        if st.session_state.llm_explanation:
            explanation_text = st.session_state.llm_explanation
            
            sections = explanation_text.split("##")
            if len(sections) > 1:
                for idx, section in enumerate(sections[1:], 1):
                    lines = section.strip().split("\n", 1)
                    if len(lines) >= 2:
                        header = lines[0].strip()
                        content = lines[1].strip()
                        
                        if "overall" in header.lower() or "interpretation" in header.lower():
                            icon = "ðŸ“‹"
                            expanded = True
                        elif "audio" in header.lower():
                            icon = "ðŸŽ™ï¸"
                            expanded = True
                        elif "text" in header.lower() or "language" in header.lower():
                            icon = "ðŸ’¬"
                            expanded = True
                        elif "multimodal" in header.lower() or "integration" in header.lower():
                            icon = "ðŸ”"
                            expanded = True
                        elif "insight" in header.lower():
                            icon = "ðŸ’¡"
                            expanded = False
                        elif "recommend" in header.lower():
                            icon = "âœ¨"
                            expanded = True
                        elif "reminder" in header.lower() or "critical" in header.lower():
                            icon = "âš ï¸"
                            expanded = False
                        else:
                            icon = "ðŸ“„"
                            expanded = False
                        
                        with st.expander(f"{icon} {header}", expanded=expanded):
                            if "recommend" in header.lower():
                                st.markdown(
                                    f"<div style='background:#f0f9ff; padding:15px; border-radius:8px;'>{content}</div>",
                                    unsafe_allow_html=True
                                )
                            elif "reminder" in header.lower() or "critical" in header.lower():
                                st.warning(content)
                            else:
                                st.markdown(content)
            else:
                st.markdown(explanation_text)
        
        st.markdown("---")
        
        # Crisis Warning
        if score >= 15:
            st.markdown("### ðŸš¨ Urgent: Immediate Support Recommended")
            st.error(f"""
            **Your PHQ-8 score of {score:.1f} indicates {sev.lower()} symptoms that require prompt professional attention.**
            
            **Immediate Actions:**
            - ðŸ“ž **Call 988** (Suicide & Crisis Lifeline) if experiencing suicidal thoughts
            - ðŸ¥ **Visit Emergency Department** if you feel you cannot keep yourself safe
            - ðŸ‘¨â€âš•ï¸ **Contact mental health provider** within 24-48 hours
            - ðŸ‘¥ **Reach out to trusted person** - do not stay alone
            
            **Malaysian Crisis Support:**
            - ðŸ“ž **Befrienders KL:** 03-7956 8145
            - ðŸ“ž **MIASA Hotline:** 1-800-18-0066
            - ðŸ“ž **Talian Kasih:** 15999
            """)
        
        # Disclaimer
        st.markdown("---")
        st.markdown("### âš ï¸ Important Medical Disclaimer")
        st.info("""
        **Please read carefully:**
        - ðŸ”¬ This is a **screening tool with XAI for research purposes only**
        - âŒ This is **NOT a medical diagnosis** despite SHAP transparency
        - ðŸ‘¨â€âš•ï¸ Always consult a **licensed mental health professional**
        - ðŸ“Š SHAP values explain model decisions, not clinical reality
        - ðŸ”’ Your data is processed locally and **not stored**
        - ðŸ†˜ **If in crisis:** Call emergency services immediately
        
        **XAI Transparency:** SHAP values show how the AI model made its decision based on learned patterns from training data. This increases transparency but does not replace clinical judgment.
        """)
        
        # Reset
        st.markdown("---")
        if st.button("ðŸ”„ Start New Assessment", use_container_width=True):
            st.session_state.result = None
            st.session_state.llm_explanation = None
            st.session_state.shap_values = None
            st.rerun()

# ==================== PAGE 1: INTRODUCTION ====================
elif page == "ðŸ“Š Introduction":
    st.title("ðŸ§  Multimodal Depression Severity Assessment System")
    
    # 1. Simplified Introduction
    st.markdown("""
    <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                padding: 2rem; border-radius: 15px; color: white; margin-bottom: 2rem;'>
        <h2 style='color: white; margin-top: 0;'>LLM-Based Insight Engine</h2>
        <p style='font-size: 1.1rem;'>
            Depression is a serious mental disorder characterized by persistent sadness, loss of interest, and a decline in daily functioning. 
            Globally, it is a critical public health concern, yet traditional screening methods often face barriers like stigma, cost, and limited accessibility. 
            This system leverages <b>Multimodal AI (Text + Audio)</b> to provide a transparent, accessible, and explainable estimation of depression severity based on the PHQ-8 scale.
        </p>
    </div>
    """, unsafe_allow_html=True)

    # 2. PHQ-8 Score Explanation
    st.markdown("### ðŸ“‰ Understanding the PHQ-8 Scale")
    st.markdown("The **Patient Health Questionnaire (PHQ-8)** is a standard diagnostic tool used to measure the severity of depressive symptoms. Scores range from 0 to 24.")
    
    # Create a clean visual for the scores
    phq_data = {
        "Score Range": ["0 - 4", "5 - 9", "10 - 14", "15 - 19", "20 - 24"],
        "Severity Level": ["None / Minimal", "Mild", "Moderate", "Moderately Severe", "Severe"],
        "Description": [
            "No significant depressive symptoms.",
            "Symptoms are present but may not require immediate treatment.",
            "Symptoms are clinically significant; counseling often recommended.",
            "Warrants active treatment with psychotherapy or medication.",
            "Severe symptoms requiring immediate professional intervention."
        ]
    }
    st.table(pd.DataFrame(phq_data).set_index("Score Range"))

    st.divider()

    # 3. Problem Statement & Objectives
    col_prob, col_obj = st.columns(2)

    with col_prob:
        st.markdown("### ðŸš© Problem Statement")
        st.info("""
        **1. Barriers to Clinical Diagnosis** Traditional in-person assessments often discourage help-seeking due to high costs, stigma, and long waiting times, leaving over 60% of cases undiagnosed.

        **2. Lack of AI Interpretability** Many existing AI models act as "black boxes," providing scores without explanation, which reduces clinician trust and transparency.

        **3. Limitations of Unimodal Systems** Systems relying solely on text or audio fail to capture the full emotional and linguistic context needed for accurate mental health assessment.
        """)

    with col_obj:
        st.markdown("### ðŸŽ¯ Project Objectives")
        st.success("""
        **1. Develop Multimodal Model** To build a robust system that fuses textual embeddings (DepRoBERTa) and acoustic features (HuBERT) to accurately predict PHQ-8 scores.

        **2. Evaluate Performance** To rigorously test the model using standard metrics: Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Concordance Correlation Coefficient (CCC).

        **3. Deploy Explainable AI** To integrate Explainable AI (SHAP/LIME) and LLM-based feedback into a user-friendly interface, providing transparent and personalized insights.
        """)

    st.divider()

    # 4. Impact
    st.markdown("### ðŸš€ Real-World Impact")
    st.markdown("This technology is designed to integrate seamlessly into the modern healthcare ecosystem.")

    im1, im2, im3 = st.columns(3)
    
    with im1:
        st.markdown("#### ðŸ¥ For Rural & Remote Clinics")
        st.info("""
        **The Problem:** Rural areas often lack mental health specialists.
        
        **The Impact:** This tool empowers general practitioners (GPs) or nurses to perform specialist-level screening instantly, acting as a triage tool to prioritize urgent cases.
        """)

    with im2:
        st.markdown("#### ðŸ“± For Telemedicine & Apps")
        st.info("""
        **The Problem:** Video calls miss subtle non-verbal cues.
        
        **The Impact:** Integrated into telehealth platforms, this AI creates an "Objective Dashboard" for therapists, alerting them to acoustic markers of distress that human ears might miss during a short call.
        """)

    with im3:
        st.markdown("#### ðŸŽ“ For Universities & Corporate")
        st.info("""
        **The Problem:** High burnout and "hidden" depression.
        
        **The Impact:** Provides a private, stigma-free self-assessment tool for students and employees, encouraging early help-seeking before burnout becomes a crisis.
        """)

    st.divider()
    
    # 4. Our Approach
    st.markdown("### ðŸ’¡ Our Approach")
    st.markdown("""
    We combine advanced deep learning with explainability:
    - **Text Analysis:** Uses **DepRoBERTa-large** with BiGRU to detect linguistic patterns and sentiment.
    - **Audio Analysis:** Uses **HuBERT** embeddings with SVR to capture prosodic features and vocal markers.
    - **Fusion Model:** Applies a **Huber Regression** to optimize the combination of text and audio signals.
    - **Insight Engine:** Generates natural language explanations using **LLMs**, **SHAP** and **LIME** values.
    
    *Performance Achievement:* **0.52 CCC** and **4.15 MAE** on test data.
    """)

    # 5. System Architecture (Existing)
    st.markdown("### ðŸ—ï¸ System Architecture")
    arch_col1, arch_col2, arch_col3 = st.columns(3)
    
    with arch_col1:
        st.info("""
        **ðŸ“ Text Module**
        - DepRoBERTa-large encoder
        - BiGRU with attention
        - Sentiment & linguistic analysis
        """)
    
    with arch_col2:
        st.success("""
        **ðŸŽ¤ Audio Module**
        - HuBERT embeddings
        - SVR regression
        - Acoustic feature extraction
        """)
    
    with arch_col3:
        st.warning("""
        **ðŸ”— Fusion Layer**
        - Huber Regression
        """)

    st.divider()

    # 6. Clinical Relevance
    st.markdown("### ðŸ’™ Clinical Relevance & Ethics")
    
    disclaim_col, usecase_col = st.columns(2)
    
    with disclaim_col:
        st.error("""
        **âš ï¸ Important Disclaimers**
        - This tool is for **screening and research purposes only**.
        - **Not a replacement** for professional mental health diagnosis.
        - Always consult a **licensed mental health professional**.
        - **If in crisis, call 988** (Suicide & Crisis Lifeline).
        """)
    
    with usecase_col:
        st.success("""
        **âœ… Use Cases**
        - Early screening in primary care settings.
        - Monitoring treatment progress over time.
        - Research and population health studies.
        - Self-awareness and wellness tracking (supplementary).
        """)

# ==================== PAGE 2: DASHBOARD ====================
elif page == "ðŸ“ˆ Dashboard":
    st.title("ðŸ“Š Model Performance Analytics")
    st.markdown("### ðŸ† The Power of Multimodal Fusion")
    st.markdown("Evidence that combining **Voice (How you say it)** and **Language (What you say)** outperforms single-modality approaches.")
    
    st.markdown("---")

    # 1. KEY METRICS ROW\
    c1, c2, c3 = st.columns(3)
    
    with c1:
        st.metric(
            label="ðŸ“ Text-Only Model (Test)",
            value="0.406 CCC",
            help="Concordance Correlation Coefficient on Test Set"
        )
        st.caption("MAE: 4.61 | RMSE: 5.60")
        
    with c2:
        st.metric(
            label="ðŸŽ¤ Audio-Only Model (Test)",
            value="0.433 CCC",
            help="Concordance Correlation Coefficient on Test Set"
        )
        st.caption("MAE: 4.87 | RMSE: 6.13")
        
    with c3:
        st.metric(
            label="ðŸš€ Fusion Model (Best)",
            value="0.520 CCC",
            delta="+20.1% vs Audio Model",
            delta_color="normal" # Green for positive
        )
        st.caption("**MAE: 4.15** (Lowest Error) | **RMSE: 5.13**")

    st.markdown("---")

    # 2. MODEL COMPARISON (Bar Chart)
    st.markdown("### ðŸ“‰ Comparative Benchmarks")
    
    # Data Preparation
    data = {
        "Model": ["Text Only", "Audio Only", "Fusion (Ours)"],
        "CCC": [0.406, 0.433, 0.520],
        "MAE": [4.61, 4.87, 4.15],
        "RMSE": [5.60, 6.13, 5.13]
    }
    df_perf = pd.DataFrame(data)
    fusion_color = "#10b981"  # Emerald Green
    unimodal_color = "#94a3b8" # Slate Gray

    tab_ccc, tab_mae, tab_rmse = st.tabs(["ðŸŽ¯ Accuracy (CCC)", "ðŸ“‰ Mean Error (MAE)", "ðŸ“ Peak Error (RMSE)"])

    with tab_ccc:
        st.markdown("#### Concordance Correlation Coefficient (Higher is Better)")
        fig_ccc = px.bar(
            df_perf, x="Model", y="CCC", text="CCC",
            color="Model", color_discrete_map={"Text Only": unimodal_color, "Audio Only": unimodal_color, "Fusion (Ours)": fusion_color}
        )
        fig_ccc.update_layout(showlegend=False, template="plotly_white", yaxis_title="CCC Value")
        st.plotly_chart(fig_ccc, use_container_width=True)

    with tab_mae:
        st.markdown("#### Mean Absolute Error (Lower is Better)")
        fig_mae = px.bar(
            df_perf, x="Model", y="MAE", text="MAE",
            color="Model", color_discrete_map={"Text Only": unimodal_color, "Audio Only": unimodal_color, "Fusion (Ours)": "#059669"}
        )
        fig_mae.update_layout(showlegend=False, template="plotly_white", yaxis_title="MAE Value")
        st.plotly_chart(fig_mae, use_container_width=True)

    with tab_rmse:
        st.markdown("#### Root Mean Square Error (Lower is Better)")
        fig_rmse = px.bar(
            df_perf, x="Model", y="RMSE", text="RMSE",
            color="Model", color_discrete_map={"Text Only": unimodal_color, "Audio Only": unimodal_color, "Fusion (Ours)": "#047857"}
        )
        fig_rmse.update_layout(showlegend=False, template="plotly_white", yaxis_title="RMSE Value")
        st.plotly_chart(fig_rmse, use_container_width=True)

    st.markdown("---")

    # 3. PREDICTED VS ACTUAL (The New Graph)
    st.markdown("### ðŸŽ¯ Accuracy Visualization: Predicted vs. Actual Scores")
    st.markdown("This scatter plot demonstrates the correlation between our AI's predictions and the clinical ground truth (PHQ-8).")

    df_real = pd.read_csv("fuse_test_prediction.csv")
    
    df_scatter = pd.DataFrame({
            "Actual Score (Ground Truth)": df_real['gt'], 
            "AI Predicted Score": df_real['prediction'],    
            # Calculate severity labels for coloring
            "Severity": ["Severe" if x>=20 else "Mod-Severe" if x>=15 else "Moderate" if x>=10 else "Mild" if x>=5 else "Normal" for x in df_real['gt']]
    })
    
    fig_scatter = px.scatter(
        df_scatter, 
        x="Actual Score (Ground Truth)", 
        y="AI Predicted Score",
        color="Severity",
        color_discrete_sequence=['#10b981', '#3b82f6', '#f59e0b', '#ef4444', '#7f1d1d'],
        hover_data=["Actual Score (Ground Truth)", "AI Predicted Score"],
        title="Correlation Analysis (Test Set)"
    )
    
    # Add Perfect Prediction Line (Diagonal)
    fig_scatter.add_shape(
        type="line", line=dict(dash="dash", color="gray", width=1),
        x0=0, y0=0, x1=24, y1=24
    )
    
    fig_scatter.update_layout(
        height=500,
        xaxis_title="Clinical Diagnosis (PHQ-8)",
        yaxis_title="AI Model Prediction",
        template="plotly_white"
    )
    
    st.plotly_chart(fig_scatter, use_container_width=True)
    
    st.info("""
    **ðŸ“ˆ How to read this graph:**
    - The **Dashed Line** represents perfect prediction (100% accuracy).
    - Points closer to the line indicate **higher accuracy**.
    - Our Fusion Model (Dots) aligns with the diagonal, validating its clinical utility across different severity levels in real life.
    """)
    
